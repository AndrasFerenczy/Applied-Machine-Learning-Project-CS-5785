{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Load Datasets\n",
    "Load train and validation splits.\n"
   ],
   "id": "191eb4dc45dcdc87"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "\n",
    "df_train = pd.read_csv(\"data/splits/train.csv\")\n",
    "df_val   = pd.read_csv(\"data/splits/val.csv\")\n",
    "df_test = pd.read_csv(\"data/splits/test.csv\")\n",
    "\n",
    "print(\"Train shape:\", df_train.shape)\n",
    "print(\"Val shape:\", df_val.shape)\n",
    "print(\"Test shape:\", df_test.shape)\n",
    "\n",
    "feature_cols = [\n",
    "    \"day_of_month\",\n",
    "    \"is_weekend\",\n",
    "    \"is_public_holiday\",\n",
    "    \"days_from_summer_start\",\n",
    "    \"flight_day_of_week\",\n",
    "    \"flight_month\",\n",
    "]\n",
    "\n",
    "target_cols = [c for c in df_train.columns if \"days to departure\" in c]\n",
    "target_cols = sorted(target_cols, key=lambda x: int(x.split()[0]))\n",
    "\n",
    "X_train = df_train[feature_cols].values\n",
    "X_val   = df_val[feature_cols].values\n",
    "X_test  = df_test[feature_cols].values\n",
    "\n",
    "y_train = df_train[target_cols].values\n",
    "y_val   = df_val[target_cols].values\n",
    "y_test  = df_test[target_cols].values\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"X_val shape:\", X_val.shape)\n",
    "print(\"y_val shape:\", y_val.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n"
   ],
   "id": "ccee51a88c07336b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Train Model\n",
    "Train a XGBoost regression model to predict flight prices for all 60 days-to-departure horizons.\n"
   ],
   "id": "eaa4ff6a4d729bff"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "base_model = XGBRegressor(\n",
    "    max_depth=100,\n",
    "    learning_rate=0.001,\n",
    "    n_estimators=600,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    objective=\"reg:squarederror\",\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "model = MultiOutputRegressor(base_model)\n",
    "\n",
    "print(\"Training model...\")\n",
    "model.fit(X_train, y_train)\n",
    "print(\"Training completed.\")\n",
    "\n",
    "y_val_pred = model.predict(X_val)\n",
    "print(\"Prediction completed.\")"
   ],
   "id": "9cebdb3c40da4b77"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Evaluate and Visualize Results\n",
    "Calculate MAE metrics and create visualizations comparing predictions to ground truth across different days-to-departures.\n"
   ],
   "id": "5c47290921e6f831"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def evaluate_and_visualize_predictions(y_val, y_val_pred, target_cols, model_name=\"Model\"):\n",
    "    n_samples, n_outputs = y_val.shape\n",
    "    comparison_data = []\n",
    "\n",
    "    for sample_idx in range(n_samples):\n",
    "        for output_idx in range(n_outputs):\n",
    "            actual = y_val[sample_idx, output_idx]\n",
    "            predicted = y_val_pred[sample_idx, output_idx]\n",
    "            abs_error = abs(actual - predicted)\n",
    "\n",
    "            days_to_departure = int(target_cols[output_idx].split()[0])\n",
    "\n",
    "            comparison_data.append({\n",
    "                \"sample_idx\": sample_idx,\n",
    "                \"days_to_departure\": days_to_departure,\n",
    "                \"actual\": actual,\n",
    "                \"predicted\": predicted,\n",
    "                \"absolute_error\": abs_error\n",
    "            })\n",
    "\n",
    "    comparison_df = pd.DataFrame(comparison_data)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"PREDICTION COMPARISON SUMMARY - {model_name}\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    mae_per_output = [\n",
    "        mean_absolute_error(y_val[:, i], y_val_pred[:, i])\n",
    "        for i in range(n_outputs)\n",
    "    ]\n",
    "\n",
    "    overall_mae = mean_absolute_error(y_val, y_val_pred)\n",
    "\n",
    "    print(f\"\\nOverall MAE: {overall_mae:.2f}\")\n",
    "    print(f\"Total predictions: {len(comparison_df)}\")\n",
    "    print(f\"Number of samples: {n_samples}\")\n",
    "    print(f\"Outputs per sample: {n_outputs}\")\n",
    "    print(\"\\nFirst rows:\")\n",
    "    print(comparison_df.head(20))\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    fig.suptitle(f'{model_name} Performance Evaluation: Predictions vs Ground Truth',\n",
    "                 fontsize=16, fontweight='bold')\n",
    "\n",
    "    ax1 = axes[0]\n",
    "    scatter = ax1.scatter(\n",
    "        comparison_df['actual'],\n",
    "        comparison_df['predicted'],\n",
    "        alpha=0.5,\n",
    "        s=20,\n",
    "        c=comparison_df['days_to_departure'],\n",
    "        cmap='viridis',\n",
    "        edgecolors='none'\n",
    "    )\n",
    "\n",
    "    min_val = min(comparison_df['actual'].min(), comparison_df['predicted'].min())\n",
    "    max_val = max(comparison_df['actual'].max(), comparison_df['predicted'].max())\n",
    "    ax1.plot([min_val, max_val], [min_val, max_val], 'r--', lw=2, label='Perfect Prediction')\n",
    "\n",
    "    ax1.set_xlabel('Actual Values', fontsize=12)\n",
    "    ax1.set_ylabel('Predicted Values', fontsize=12)\n",
    "    ax1.set_title('Predictions vs Actual (colored by days to departure)', fontsize=12, fontweight='bold')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    plt.colorbar(scatter, ax=ax1, label='Days to Departure')\n",
    "\n",
    "    ax2 = axes[1]\n",
    "    days_list = [int(col.split()[0]) for col in target_cols]\n",
    "\n",
    "    ax2.plot(days_list, mae_per_output, marker='o', linewidth=2, markersize=6, color='steelblue')\n",
    "    ax2.set_xlabel('Days to Departure', fontsize=12)\n",
    "    ax2.set_ylabel('MAE', fontsize=12)\n",
    "    ax2.set_title('MAE by Days to Departure', fontsize=12, fontweight='bold')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.set_xticks(range(0, 61, 10))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return comparison_df, overall_mae, mae_per_output\n",
    "\n",
    "\n",
    "# Call the function with current data\n",
    "comparison_df, overall_mae, mae_per_output = evaluate_and_visualize_predictions(\n",
    "    y_val, y_val_pred, target_cols, model_name=\"XGBoost\"\n",
    ")"
   ],
   "id": "44fd8ef75b21f50b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Hyperparamter Tuning\n",
    "Find the best max depth, num estimators, learning rate for optimal results with grid searh"
   ],
   "id": "1fbc4027b102039a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_absolute_error, make_scorer\n",
    "from xgboost import XGBRegressor\n",
    "import numpy as np\n",
    "\n",
    "base_xgb = XGBRegressor(\n",
    "    objective=\"reg:squarederror\",\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "multioutput_xgb = MultiOutputRegressor(base_xgb)\n",
    "\n",
    "# the Param grid\n",
    "param_grid = {\n",
    "    \"estimator__max_depth\": [3, 5, 7, 10, 15, 20],\n",
    "    \"estimator__learning_rate\": [0.03, 0.05, 0.1],\n",
    "    \"estimator__n_estimators\": [100, 200, 300, 400, 500, 600],\n",
    "    \"estimator__subsample\": [0.7, 1.0],\n",
    "    \"estimator__colsample_bytree\": [0.7, 1.0],\n",
    "    \"estimator__reg_lambda\": [1.0, 2.0],\n",
    "    \"estimator__reg_alpha\": [0.0, 1.0],\n",
    "}\n",
    "\n",
    "mae_scorer = make_scorer(mean_absolute_error, greater_is_better=False)\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=multioutput_xgb,\n",
    "    param_grid=param_grid,\n",
    "    scoring=mae_scorer,\n",
    "    cv=3,\n",
    "    n_jobs=-1,\n",
    "    verbose=2,\n",
    ")\n",
    "\n",
    "print(\"Running grid search...\")\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(\"Grid search done.\")\n",
    "\n",
    "print(\"\\nBest params:\")\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "print(f\"\\nBest CV MAE (mean over folds): {-grid_search.best_score_:.4f}\")\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "y_val_pred = best_model.predict(X_val)\n",
    "val_mae = mean_absolute_error(y_val, y_val_pred)\n",
    "print(f\"Validation MAE with best params: {val_mae:.4f}\")\n"
   ],
   "id": "ba11649630c51a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "evaluate_and_visualize_predictions(\n",
    "    y_val, y_val_pred, target_cols, model_name=\"XGBoost (Optimal Hyperparameters)\"\n",
    ")\n",
    "print(\"\")"
   ],
   "id": "b3558d75c946d5f6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Comparison with Ridge Regression\n",
    "A simpler linear regression model is trained to compare it with XGBoost's performance as reference."
   ],
   "id": "192fb966a0f2e601"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "\n",
    "ridge_base = Ridge(alpha=1.0, random_state=42)  \n",
    "ridge_model = MultiOutputRegressor(ridge_base)\n",
    "ridge_model.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred_ridge = ridge_model.predict(X_train)\n",
    "y_val_pred_ridge = ridge_model.predict(X_val)\n",
    "\n",
    "ridge_train_mae = mean_absolute_error(y_train, y_train_pred_ridge)\n",
    "ridge_val_mae = mean_absolute_error(y_val, y_val_pred_ridge)\n",
    "\n",
    "xgb_baseline_base = XGBRegressor(\n",
    "    max_depth=100,\n",
    "    learning_rate=0.001,\n",
    "    n_estimators=600,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    objective=\"reg:squarederror\",\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "xgb_baseline_model = MultiOutputRegressor(xgb_baseline_base)\n",
    "xgb_baseline_model.fit(X_train, y_train)\n",
    "\n",
    "y_val_pred_xgb_baseline = xgb_baseline_model.predict(X_val)\n",
    "xgb_baseline_val_mae = mean_absolute_error(y_val, y_val_pred_xgb_baseline)\n",
    "\n",
    "if 'model' not in globals() or model is None:\n",
    "    print(\"\\nTraining XGBoost model with optimal hyperparameters...\")\n",
    "    xgb_optimal_base = XGBRegressor(\n",
    "        max_depth=3,\n",
    "        learning_rate=0.03,\n",
    "        n_estimators=100,\n",
    "        subsample=0.7,\n",
    "        colsample_bytree=1.0,\n",
    "        reg_lambda=2.0,\n",
    "        reg_alpha=0.0,\n",
    "        objective=\"reg:squarederror\",\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "    model = MultiOutputRegressor(xgb_optimal_base)\n",
    "    model.fit(X_train, y_train)\n",
    "    print(\"XGBoost (Optimal) training completed!\")\n",
    "\n",
    "y_val_pred_xgb_optimal = model.predict(X_val)\n",
    "xgb_optimal_val_mae = mean_absolute_error(y_val, y_val_pred_xgb_optimal)\n",
    "\n",
    "n_samples, n_outputs = y_val.shape\n",
    "\n",
    "ridge_mae_per_output = [\n",
    "    mean_absolute_error(y_val[:, i], y_val_pred_ridge[:, i]) \n",
    "    for i in range(n_outputs)\n",
    "]\n",
    "\n",
    "xgb_baseline_mae_per_output = [\n",
    "    mean_absolute_error(y_val[:, i], y_val_pred_xgb_baseline[:, i]) \n",
    "    for i in range(n_outputs)\n",
    "]\n",
    "\n",
    "xgb_optimal_mae_per_output = [\n",
    "    mean_absolute_error(y_val[:, i], y_val_pred_xgb_optimal[:, i]) \n",
    "    for i in range(n_outputs)\n",
    "]\n",
    "\n",
    "days_list = [int(col.split()[0]) for col in target_cols]\n",
    "\n",
    "print(f\"Ridge - Mean MAE: {np.mean(ridge_mae_per_output):.2f}, \"\n",
    "      f\"Min: {np.min(ridge_mae_per_output):.2f}, \"\n",
    "      f\"Max: {np.max(ridge_mae_per_output):.2f}\")\n",
    "print(f\"XGBoost (Baseline) - Mean MAE: {np.mean(xgb_baseline_mae_per_output):.2f}, \"\n",
    "      f\"Min: {np.min(xgb_baseline_mae_per_output):.2f}, \"\n",
    "      f\"Max: {np.max(xgb_baseline_mae_per_output):.2f}\")\n",
    "print(f\"XGBoost (Optimal) - Mean MAE: {np.mean(xgb_optimal_mae_per_output):.2f}, \"\n",
    "      f\"Min: {np.min(xgb_optimal_mae_per_output):.2f}, \"\n",
    "      f\"Max: {np.max(xgb_optimal_mae_per_output):.2f}\")\n",
    "\n",
    "# Create comparison plot: MAE vs Days to Departure\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(days_list, ridge_mae_per_output, marker='o', linewidth=2, markersize=5, \n",
    "         label='Ridge Regression', color='steelblue', alpha=0.8)\n",
    "plt.plot(days_list, xgb_baseline_mae_per_output, marker='^', linewidth=2, markersize=5, \n",
    "         label='XGBoost (Baseline)', color='green', alpha=0.8)\n",
    "plt.plot(days_list, xgb_optimal_mae_per_output, marker='s', linewidth=2, markersize=5, \n",
    "         label='XGBoost (Optimal Hyperparameters)', color='coral', alpha=0.8)\n",
    "\n",
    "plt.xlabel('Days to Departure', fontsize=12)\n",
    "plt.ylabel('MAE (Mean Absolute Error)', fontsize=12)\n",
    "plt.title('Model Comparison: MAE by Days to Departure', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xticks(range(0, 61, 10))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "7b1d7e2f703b8f2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Running on the Test Set\n",
   "id": "db69cfd5861b40e5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "y_test_pred_xgb_optimal = model.predict(X_test)\n",
    "xgb_optimal_test_mae = mean_absolute_error(y_test, y_test_pred_xgb_optimal)\n",
    "\n",
    "print(f\"XGBoost Test MAE: {xgb_optimal_test_mae:.2f}\")"
   ],
   "id": "de4bea589da4e46f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Results: MAE of 52 dollars on the Test set!",
   "id": "d358c07b70032da7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "a4bcb8090f64d680"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
